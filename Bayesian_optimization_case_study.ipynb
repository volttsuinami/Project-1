{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNK7vbHo-KYU"
   },
   "source": [
    "## Bayesian methods of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlFdvPwF-KYW"
   },
   "source": [
    "In addition to the random search and the grid search methods for selecting optimal hyperparameters, we can use Bayesian methods of probabilities to select the optimal hyperparameters for an algorithm.\n",
    "\n",
    "In this case study, we will be using the BayesianOptimization library to perform hyperparmater tuning. This library has very good documentation which you can find here: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "You will need to install the Bayesian optimization module. Running a cell with an exclamation point in the beginning of the command will run it as a shell command — please do this to install this module from our notebook in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pssx080d-Ulf"
   },
   "outputs": [],
   "source": [
    "\n",
    "#! pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:39:09.312682Z",
     "start_time": "2019-04-22T16:39:09.309208Z"
    },
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "l9nfFTyj-KYY"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from bayes_opt import BayesianOptimization\n",
    "from catboost import CatBoostClassifier, cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "D16Dquw1AAK0",
    "outputId": "44167587-f22e-4bf5-a816-e2bcfdc6c4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " 'Bayesian_optimization_case_study.ipynb',\n",
       " 'flight_delays_test.csv',\n",
       " 'flight_delays_train.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:48:15.929012Z",
     "start_time": "2019-04-22T14:48:15.926574Z"
    },
    "colab_type": "text",
    "id": "AkBt3yds-KYu"
   },
   "source": [
    "## How does Bayesian optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1kyBCUs-KYv"
   },
   "source": [
    "Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAdHF72R-KYw"
   },
   "source": [
    "<img src=\"https://github.com/fmfn/BayesianOptimization/blob/master/examples/bo_example.png?raw=true\" />\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB — aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored (see the gif below).\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bayesian_optimization.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTP8KUlLoYzu"
   },
   "source": [
    "## Let's look at a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crpPqKdC-KYx"
   },
   "source": [
    "The first step is to create an optimizer. It uses two items:\n",
    "* function to optimize\n",
    "* bounds of parameters\n",
    "\n",
    "The function is the procedure that counts metrics of our model quality. The important thing is that our optimization will maximize the value on function. Smaller metrics are best. Hint: don't forget to use negative metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09ciF8gpTfr"
   },
   "source": [
    "Here we define our simple function we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofwvnfEwo5mG"
   },
   "outputs": [],
   "source": [
    "def simple_func(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGsdciCpeI3"
   },
   "source": [
    "Now, we define our bounds of the parameters to optimize, within the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jLYW2qnpOFr"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    simple_func,\n",
    "    {'a': (1, 3),\n",
    "    'b': (4, 7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg6LdYx8pq2T"
   },
   "source": [
    "These are the main parameters of this function:\n",
    "\n",
    "* **n_iter:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "\n",
    "* **init_points:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GKMJ1uqMYv"
   },
   "source": [
    "Let's run an example where we use the optimizer to find the best values to maximize the target value for a and b given the inputs of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Oy44Ro7wqNat",
    "outputId": "9cc64d54-b1e6-46d1-dc29-4c0039a1c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m8.53     \u001b[0m | \u001b[0m1.842    \u001b[0m | \u001b[0m6.688    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m7.883    \u001b[0m | \u001b[0m1.405    \u001b[0m | \u001b[0m6.479    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m7.415    \u001b[0m | \u001b[0m2.069    \u001b[0m | \u001b[0m5.346    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m8.53     \u001b[0m | \u001b[0m1.834    \u001b[0m | \u001b[0m6.696    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m9.387    \u001b[0m | \u001b[95m2.387    \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer.maximize(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyKFMF2Hq2Sx"
   },
   "source": [
    "Great, now let's print the best parameters and the associated maximized target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_H6DixyfscV_",
    "outputId": "fd0c35d7-e30d-4d30-9ab2-12c0fa837971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2.3869124902573917, 'b': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.38691249025739"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimizer.max['params']);optimizer.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ1T1V6Mspi4"
   },
   "source": [
    "## Test it on real data using the Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_oGwREZkm4h"
   },
   "source": [
    "The dataset we will be working with is the famous flight departures dataset. Our modeling goal will be to predict if a flight departure is going to be delayed by 15 minutes based on the other attributes in our dataset. As part of this modeling exercise, we will use Bayesian hyperparameter optimization to identify the best parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYSagjQANDZ"
   },
   "source": [
    "**<font color='teal'> You can load the zipped csv files just as you would regular csv files using Pandas read_csv. In the next cell load the train and test data into two seperate dataframes. </font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWKBApVuAeJe"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('flight_delays_train.csv')\n",
    "test_df = pd.read_csv('flight_delays_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OapNcT9Eikis"
   },
   "source": [
    "**<font color='teal'> Print the top five rows of the train dataframe and review the columns in the data. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "__4cXZ8iiYaC",
    "outputId": "8718ad4b-8955-486c-9ae8-1dee6aa6c2fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "\n",
       "  dep_delayed_15min  \n",
       "0                 N  \n",
       "1                 N  \n",
       "2                 N  \n",
       "3                 N  \n",
       "4                 Y  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxGBsPQhffgd"
   },
   "source": [
    "**<font color='teal'> Use the describe function to review the numeric columns in the train dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "_bRRKG3DAtae",
    "outputId": "7cfb9975-ec97-422c-abbd-98923a0b7aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.523880</td>\n",
       "      <td>729.39716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.378445</td>\n",
       "      <td>574.61686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.000000</td>\n",
       "      <td>317.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.000000</td>\n",
       "      <td>575.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.000000</td>\n",
       "      <td>957.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.000000</td>\n",
       "      <td>4962.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DepTime      Distance\n",
       "count  100000.000000  100000.00000\n",
       "mean     1341.523880     729.39716\n",
       "std       476.378445     574.61686\n",
       "min         1.000000      30.00000\n",
       "25%       931.000000     317.00000\n",
       "50%      1330.000000     575.00000\n",
       "75%      1733.000000     957.00000\n",
       "max      2534.000000    4962.00000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6k-_fI5Aiyh"
   },
   "source": [
    "Notice, `DepTime` is the departure time in a numeric representation in 2400 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZS4-hrlQah"
   },
   "source": [
    " **<font color='teal'>The response variable is 'dep_delayed_15min' which is a categorical column, so we need to map the Y for yes and N for no values to 1 and 0. Run the code in the next cell to do this.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:38:42.677690Z",
     "start_time": "2019-04-22T15:38:42.481963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yRlOTbnW-KYc"
   },
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
    "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WPkFQO9uo9"
   },
   "source": [
    "## Feature Engineering\n",
    "Use these defined functions to create additional features for the model. Run the cell to add the functions to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqsqz5W9t3r"
   },
   "outputs": [],
   "source": [
    "def label_enc(df_column):\n",
    "    df_column = LabelEncoder().fit_transform(df_column)\n",
    "    return df_column\n",
    "\n",
    "def make_harmonic_features_sin(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.sin(value)\n",
    "\n",
    "def make_harmonic_features_cos(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value)\n",
    "\n",
    "def feature_eng(df):\n",
    "    df['flight'] = df['Origin']+df['Dest']\n",
    "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
    "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
    "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
    "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
    "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
    "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
    "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
    "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
    "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
    "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
    "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
    "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
    "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
    "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
    "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
    "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
    "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
    "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
    "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
    "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
    "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
    "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
    "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
    "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
    "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
    "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
    "    return df.drop('DepTime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BYbxXpU-FGE"
   },
   "source": [
    "Concatenate the training and testing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6bfSNw_RAf"
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
    "full_df = feature_eng(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSO8JbfM_W-F"
   },
   "source": [
    "Apply the earlier defined feature engineering functions to the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RfAINftjwi"
   },
   "outputs": [],
   "source": [
    "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
    "    full_df[column] = label_enc(full_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJAw1RGB_ZuM"
   },
   "source": [
    "\n",
    "Split the new full dataframe into X_train and X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15cPtQU5tjfz"
   },
   "outputs": [],
   "source": [
    "X_train = full_df[:train_df.shape[0]]\n",
    "X_test = full_df[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umfAw-9JErLV"
   },
   "source": [
    "Create a list of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:31:58.412296Z",
     "start_time": "2019-04-22T14:31:58.409088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ibeVyNb-KZI"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMIsMPIETVk"
   },
   "source": [
    "Let's build a light GBM model to test the bayesian optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:18:04.466965Z",
     "start_time": "2019-04-22T15:18:04.457992Z"
    },
    "colab_type": "text",
    "id": "2hfm1i5G-KZH"
   },
   "source": [
    "### [LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-3F2Wg-KZL"
   },
   "source": [
    "First, we define the function we want to maximize and that will count cross-validation metrics of lightGBM for our parameters.\n",
    "\n",
    "Some params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:40:14.034265Z",
     "start_time": "2019-04-22T15:40:14.027868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LyUJBhGX-KZM"
   },
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\" : int(num_leaves),\n",
    "        \"max_depth\" : int(max_depth),\n",
    "        \"lambda_l2\" : lambda_l2,\n",
    "        \"lambda_l1\" : lambda_l1,\n",
    "        \"num_threads\" : 20,\n",
    "        \"min_child_samples\" : int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\" : 0.03,\n",
    "        \"subsample_freq\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                       lgtrain,\n",
    "                       1000,\n",
    "                       stratified=True,\n",
    "                       nfold=3)\n",
    "    print(cv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJwqBhdeF11Q"
   },
   "source": [
    "Apply the Bayesian optimizer to the function we created in the previous step to identify the best hyperparameters. We will run 10 iterations and set init_points = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:48:04.682447Z",
     "start_time": "2019-04-22T15:40:14.641634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JheCOkUE-KZP",
    "outputId": "8f37ee51-885d-44e4-cdcd-ceb7abd58b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "{'valid auc-mean': [0.6961562511237932, 0.7046413846105374, 0.7086987761730447, 0.7105667230370275, 0.7120649268379277, 0.7134524918875916, 0.7146302393078342, 0.7152355508049079, 0.7161370930732263, 0.7165121889246922, 0.7169335099158235, 0.7171035999996341, 0.7176433383434176, 0.7179045624944771, 0.7180777769209038, 0.71833727355521, 0.7183350068227, 0.718576103468563, 0.7187645543077705, 0.7188532719081017, 0.7190170096877707, 0.7189593256359599, 0.7190418086370212, 0.7193034080488427, 0.7192931008423423, 0.7192792516372113, 0.7192423471139676, 0.7192262575927927, 0.7192069395550439, 0.7191605637815011, 0.7192121789911852, 0.7191124782600982, 0.719144495357344, 0.7191376001193625, 0.719158928776495, 0.7190404506283241, 0.7189625103130028, 0.7187659583660954, 0.7187957809096844, 0.7188005809127832, 0.718616040030598, 0.7185737376888358, 0.7184293905480305, 0.7183823451425916, 0.718328816460598, 0.7183045716468106, 0.718317026457041, 0.7181482113525429, 0.7180440302772207, 0.7179996069632192, 0.7179046959469221, 0.7177601327856675, 0.7176605084033062, 0.7175348974128525, 0.7175103636039114, 0.7173821223276433, 0.7172262780537121, 0.7171697093937389, 0.7170397853299475, 0.7169801618986932, 0.7169433594476513, 0.7168064307033452, 0.7167269827644738, 0.7166499388623636, 0.7165689515868143, 0.7164301595819836, 0.7164078701466522, 0.7162755872954776, 0.7161373327880082, 0.7160618619352106, 0.715973398388433, 0.7159192776072034, 0.7158548277307863, 0.7157526359523138, 0.7155296346897848, 0.7153857936200568, 0.7152701341969302, 0.7152308557852102, 0.7150756399024735, 0.7150277448623507, 0.7149477091342572, 0.7147794913311601, 0.7146295350344136, 0.7145257935406395, 0.7144260853014233, 0.7142774391657077, 0.7141966096055404, 0.7140757584283794, 0.7139917259162974, 0.7138688864795544, 0.71374653953012, 0.7136544891120239, 0.7135633419451363, 0.7134252330232931, 0.7133151705617791, 0.7132540942969418, 0.7131906373753588, 0.7130895171750797, 0.7130033428316596, 0.7129036072183889, 0.7127935185244998, 0.7127339687772359, 0.712650346896953, 0.7125451974540377, 0.7125111248692987, 0.7123813825172447, 0.7122494665694562, 0.7122030096092017, 0.7120486496576546, 0.7119736316346579, 0.7118630871593165, 0.7117523506525623, 0.7116339681957579, 0.7115470641251234, 0.7114557814115706, 0.7113846614811602, 0.7113349031040069, 0.7112930281492842, 0.7111929676680265, 0.7110726723450127, 0.7109960685398037, 0.7108774059090898, 0.7107509327114193, 0.7107929254142782, 0.7107908279153978, 0.7107764051394329, 0.7108120502408259, 0.7107960290139247, 0.7106953344100018, 0.7106324045132366, 0.7105549042378153, 0.7104533234628051, 0.7104194447671143, 0.7103809242782567, 0.7103875472321591, 0.7102523297639974, 0.7101933596289564, 0.710121717787814, 0.7100949831849904, 0.7100667815362529, 0.7100026220706416, 0.7098663045902504, 0.7098331852641162, 0.7098009340883769, 0.7097852387401224, 0.7097414651576243, 0.7096946003492118, 0.7096903990488884, 0.7096380140768913, 0.7096051204542442, 0.7096265082941012, 0.7095451853990732, 0.7094772251007259, 0.7094330061771593, 0.7094025695243263, 0.7093265706977755, 0.7092911152713199, 0.7092453780865692, 0.7092384910701132, 0.7091941233553061, 0.7091343568665893, 0.7091005660727722, 0.7091197911612251, 0.7090931744907728, 0.7090083267382384, 0.7089608866361977, 0.7088990861099004, 0.7088579424771231, 0.7088246796447292, 0.7087602322835685, 0.7086951265258091, 0.7087047429904884, 0.7086965466816094, 0.7086826922240186, 0.7086848188886824, 0.7086457362785198, 0.7085757189450347, 0.7085729517624128, 0.7085371281020422, 0.7084620766941757, 0.7084940520947117, 0.7084455725568616, 0.7083858715976339, 0.7083739211328385, 0.708368064059302, 0.7083583305048285, 0.7083603836107525, 0.7083323191489836, 0.7082768459993475, 0.7082024159084899, 0.7081501464280944, 0.7081397183230056, 0.7081221427167409, 0.7081085022416228, 0.7080842388565296, 0.7080523094642316, 0.7080351313976984, 0.7079732468828773, 0.7079285363915903, 0.707899143339248, 0.7079104643118383, 0.7078838136672531, 0.7078522709635782, 0.7078081211191402, 0.7077102282804878, 0.7076992963235762, 0.7076477405331656, 0.7075756353816747, 0.707577360892287, 0.7075746546189335, 0.7075766859317852, 0.7075579137427891, 0.7075547943338453, 0.7075462463275533, 0.7074857819052943, 0.7074439055470698, 0.7074527357736123, 0.7074742376593864, 0.7074790658580973, 0.7073850754240639, 0.7073968578735234, 0.7073596904006703, 0.7073754716407997, 0.7073405621664629, 0.7073063966670037, 0.7073076809629754, 0.7073122502103698, 0.7072733890083183, 0.7072440569268964, 0.7071937396664287, 0.7072196272328206, 0.7071615910279491, 0.7071475089196063, 0.707147545702978, 0.7071016700947582, 0.7070766387730073, 0.7070169205298825, 0.7070042780217243, 0.7069392373727065, 0.7068676996083925, 0.7068309120348016, 0.7067932792354005, 0.7067630259451718, 0.7067179674974128, 0.7067577452985061, 0.7067569946005342, 0.7067364622074478, 0.7067121368063791, 0.70672711371961, 0.7066817341355028, 0.7066734559585677, 0.7066303250729021, 0.7066327724488398, 0.7066240340168264, 0.7066368594957925, 0.7066261626700729, 0.7066333940480777, 0.7065582595854086, 0.7065232077726661, 0.706511497615305, 0.7064572588876432, 0.7064456306214847, 0.7064506445048077, 0.7064599936650487, 0.7063832599733306, 0.7063679342211756, 0.7063120277703447, 0.706275226128494, 0.7062147859615647, 0.7062121218758296, 0.7061976648435028, 0.7061908542690799, 0.7061499441814885, 0.7061518413322124, 0.7061599887711201, 0.7061920544161607, 0.7061673860868977, 0.7061509345320133, 0.7061686652066993, 0.7061447246529866, 0.7060971574758391, 0.7061271052957393, 0.7060920949737673, 0.7060836909771905, 0.7061027329201189, 0.7061300588224332, 0.7061047950076184, 0.7061332060499746, 0.7061217021296512, 0.7060854859667677, 0.7060704533035658, 0.7060375884011494, 0.7059561707991672, 0.7059984950181152, 0.7059895381442081, 0.7060075568099856, 0.7060019953431101, 0.7060143226510324, 0.7059684479214433, 0.7058942548536639, 0.7058410400352483, 0.705843531358301, 0.705856531710971, 0.7058301659161831, 0.7058292943838708, 0.7058498015241633, 0.7058244711035097, 0.7058267631464634, 0.7058118865554199, 0.7058176765371126, 0.7058417778772327, 0.7058063079712156, 0.7058025505564696, 0.7057738585571874, 0.7057330101086653, 0.7057129470736615, 0.7057186071990582, 0.7057478687576856, 0.7057316815751292, 0.7057037597643521, 0.7056820577419852, 0.7056770794304104, 0.7056736171446848, 0.7056666587922772, 0.705662969896384, 0.7056333304568575, 0.7056271131061161, 0.7056057729023255, 0.7055985615847922, 0.7056063430605964, 0.7055816108559197, 0.7055671147845523, 0.7055910103822202, 0.7055399265916256, 0.7055157800044349, 0.7054955331749388, 0.7055022209974645, 0.705494649309996, 0.7054724667135736, 0.7054250919128014, 0.7054046592713871, 0.7054208062244788, 0.7054061168736604, 0.7053760459085882, 0.7053786905998374, 0.7053473756420966, 0.7053553046815096, 0.7053608908913785, 0.7053029290483087, 0.7053044571261459, 0.7052919551105834, 0.7053027682378173, 0.7053130932789725, 0.7053147535414594, 0.7052953829597502, 0.7052467339263396, 0.7052229674256747, 0.7051724703584076, 0.7051853552985777, 0.7051383355622818, 0.7051475919864854, 0.705105802460635, 0.705088223504036, 0.7050481775903951, 0.7050233105398668, 0.7050222403667613, 0.7049852854483629, 0.7049819306478469, 0.7049657767870657, 0.7048916772573045, 0.7048781444245185, 0.7048355067837235, 0.7048195053570575, 0.704829419417197, 0.7048259017261019, 0.7048130134512451, 0.7047204636400982, 0.7047080199170203, 0.7046966391223997, 0.7046719026816194, 0.7046455138760065, 0.704644134548741, 0.7046499235871223, 0.7045955321832805, 0.704595304547671, 0.7045955049222908, 0.7046105741960228, 0.7046152639623592, 0.7046152736230437, 0.7046034894256968, 0.7046133800957973, 0.7045585222934561, 0.7045358232687847, 0.7045257348877904, 0.7045237347255551, 0.7045575613191964, 0.7045514761520953, 0.7045904421232801, 0.704571802385948, 0.7045614152129404, 0.7045894975448411, 0.7045965830141779, 0.704577036909364, 0.7045504216351307, 0.7045467324795781, 0.7045267022519347, 0.7045035576684299, 0.7044794459322001, 0.7044800570643747, 0.7044570220714762, 0.7044412144669425, 0.7044165218293507, 0.7043966879453692, 0.7043698046686965, 0.7043560905990737, 0.7043169205255957, 0.704297149474136, 0.704264896668549, 0.7042328672834511, 0.7042632759212295, 0.7042049411454987, 0.7041517938951974, 0.7041172925513198, 0.7041386755617722, 0.7041134828109191, 0.7041020355608039, 0.7041218447134013, 0.7041036220719517, 0.7040572066662832, 0.7040526414955627, 0.7040189760235567, 0.7040225173608885, 0.7039688267699886, 0.7039707688420732, 0.7039812879076227, 0.7039586364053042, 0.7039645125162503, 0.7039779452470657, 0.703977054610068, 0.7039866047964208, 0.7039797608868104, 0.7039911850738947, 0.7040312604105265, 0.704029360853717, 0.7039855516787038, 0.7039751078473856, 0.7040187010134868, 0.704043802680359, 0.7040173152418953, 0.7040309671317865, 0.7040223975708099, 0.7040384433102486, 0.7040200974485629, 0.7040350503696565, 0.7040034653333551, 0.7040122917746103, 0.7039896889193787, 0.7040145279929613, 0.7040411414673176, 0.7040265732274441, 0.7039760449657192, 0.7039736459428861, 0.7039723633220455, 0.7039846337006047, 0.7039615911837857, 0.7039568496669245, 0.7039099951303802, 0.7039037258773996, 0.7038991848555058, 0.703885869608408, 0.7038779090972834, 0.7038697677924227, 0.7038817210716489, 0.7038725076049562, 0.7038259052772012, 0.7038412312574325, 0.7038617938328748, 0.7038980127729814, 0.7038902868564961, 0.7038923613371963, 0.7039034158761197, 0.7039019123730488, 0.7038864200508096, 0.7038963552517362, 0.7038868786337852, 0.7038366531860899, 0.7038360908424773, 0.7038510080158802, 0.7038556858916055, 0.7038212671679381, 0.7038272272739693, 0.7038167956938001, 0.703819892031709, 0.703805016095186, 0.7037588406785966, 0.7037480567931477, 0.7037539760456228, 0.703742635283974, 0.7037543554761859, 0.7037680704465767, 0.7037186301355028, 0.7037295601304394, 0.7037345160977516, 0.7037171198587183, 0.7036901343286014, 0.7036709822975241, 0.7036800735511076, 0.7036942489072356, 0.7037182719553305, 0.7037188945857787, 0.7037251197510347, 0.7037450845714113, 0.7037093130163347, 0.7037068998149169, 0.7037276485284201, 0.7037271529419281, 0.7037115780318551, 0.7037034972026079, 0.7037306809949535, 0.7037298652616458, 0.7037062599322987, 0.7036973052937984, 0.703677963350478, 0.7037026771091717, 0.7036799877569383, 0.7036829590489196, 0.7036586078678787, 0.7036303951744239, 0.703651268636411, 0.7036520914276081, 0.7036600283418896, 0.7036631896599722, 0.7036631780580075, 0.7036334878117723, 0.7036487197510354, 0.7036494335172377, 0.7036193886944062, 0.7036112043567545, 0.7036096947174011, 0.7036267307968368, 0.7036467170495317, 0.7036004991645374, 0.7035814916997015, 0.7035905048476577, 0.7035714156705902, 0.7035488666239779, 0.7035522893778072, 0.7035693513395359, 0.7035645838871664, 0.7035616651017578, 0.703566125020008, 0.7035656016509723, 0.7035673279820244, 0.7035815446074807, 0.7036030016043426, 0.7036295025918918, 0.703640339232324, 0.703653289373317, 0.7036942269307936, 0.7037079006648094, 0.7037537921374168, 0.7037648514847614, 0.7037612223044679, 0.7037614034816718, 0.7037715570599558, 0.703751582710563, 0.7037501093287729, 0.7037535889496641, 0.7037336144662234, 0.7037637115446213, 0.703737309749045, 0.7037233326469973, 0.7037087487464403, 0.70369566279595, 0.7037095152240731, 0.703702099639409, 0.7036724643157107, 0.7036497000264014, 0.7036157385791925, 0.7036601025630095, 0.7036377255836742, 0.7036405919305077, 0.7036100959734126, 0.703595953043989, 0.7035812675305193, 0.7035556560691508, 0.703547152504704, 0.7035099789502982, 0.7034982902377297, 0.7034700349675646, 0.7034719887962123, 0.7034877058571389, 0.7034888129059139, 0.7034951158715653, 0.7034904907388535, 0.7034828671027086, 0.703486941933917, 0.7034964378652327, 0.7034530840119858, 0.7034294319300342, 0.703416561615804, 0.7034135065008581, 0.703402664679713, 0.7033879813308407, 0.7033938519799983, 0.7033770884182339, 0.7033812311734797, 0.703381822448382, 0.7033748953993145, 0.7033482721875516, 0.7033640376012942, 0.7033283367130828, 0.7033242116811578, 0.7033246104831323, 0.703306223562889, 0.703284509785329, 0.7032751020440288, 0.7032858877553855, 0.7032747378541947, 0.7032801280651477, 0.7032739596987403, 0.7032643824003717, 0.7032387244944424, 0.7032501253573941, 0.7032386152701425, 0.7032126130851551, 0.703223947827552, 0.7032082951640977, 0.7032034695696602, 0.7032017163963468, 0.7032062679119119, 0.7032478937086445, 0.7032556037692509, 0.7032266726363018, 0.7032439647575494, 0.7032460704796496, 0.7032276197870345, 0.7032346129648047, 0.7032392226429164, 0.7032211302689921, 0.7032051080523424, 0.703212241220912, 0.7031961251918748, 0.7031962341333688, 0.7031707161982648, 0.703150839206212, 0.7031454882934547, 0.7031403940396778, 0.7031187500455212, 0.7031085848000874, 0.7030861488008463, 0.703085493375801, 0.7031054889718348, 0.7030899035342001, 0.7030874480440693, 0.7030655724144239, 0.7030560064582886, 0.70306666342907, 0.7030916594567588, 0.7030892990312355, 0.7030432439391862, 0.7030439231546116, 0.7030243904822852, 0.7030180762074982, 0.7029972435030919, 0.7029800829765821, 0.7029727333105714, 0.702955333808382, 0.7029410106068488, 0.7029336478962896, 0.7029273470760549, 0.7029406816481268, 0.7029112683412718, 0.7029125643515516, 0.7029259422740486, 0.7028958298824536, 0.7028942483623036, 0.7029178091429422, 0.7029275091332864, 0.702927386387302, 0.7029491742870504, 0.7029579615102935, 0.7029555778960558, 0.7029703234868888, 0.7029711797927493, 0.7029489384975163, 0.7029406087067, 0.70293084452951, 0.7029242576118007, 0.7029200178788612, 0.7029226488617394, 0.7029204930228555, 0.7029046247870284, 0.7028868492763573, 0.702888269622954, 0.702884023897731, 0.702883710866779, 0.7028945944100881, 0.7028740227576721, 0.7028405535478468, 0.702874397929634, 0.7028570020108305, 0.7028594344662685, 0.702856103219187, 0.7028597931229278, 0.7028476508659004, 0.7028325835413082, 0.7028272479202601, 0.7028263219587708, 0.702824158421322, 0.7028175737405338, 0.7028289186262312, 0.7028324115742607, 0.7028509364529514, 0.7028614211959443, 0.7028522155991442, 0.7028697402916206, 0.70285326079844, 0.7028950681382504, 0.7028994275401277, 0.7028866490045629, 0.702860827461525, 0.7028542698201399, 0.702882920538268, 0.7028985134515112, 0.7029156328524033, 0.7029210324753418, 0.7029083666058433, 0.7028930958807091, 0.7028892238398684, 0.7029023779693686, 0.7029075322310692, 0.7029066547777877, 0.7028827149969962, 0.7028583668190915, 0.7028695904318182, 0.7028772569455786, 0.7028708744669766, 0.702887398820082, 0.7028820302587794, 0.7028908570879014, 0.702904269942264, 0.7028964402820149, 0.7029001703134768, 0.7028982631646773, 0.7028858993571263, 0.7028673321806371, 0.7028643884558141, 0.7028604388609695, 0.7028912963420811, 0.7029215580341402, 0.7029311509081775, 0.7029340624933562, 0.7029111325861436, 0.7029186415700908, 0.7029047265102656, 0.702897096979615, 0.7029089393278269, 0.7029066803227614, 0.7028842754954807, 0.7028829927770784, 0.7028585173252663, 0.7028635672597088, 0.70284188652357, 0.7028592920318878, 0.7028663319138064, 0.7028614732214873, 0.7028356887857283, 0.7028015895420979, 0.702773075260659, 0.7027656304131545, 0.702728960391207, 0.702701609164424, 0.7026913370027817, 0.7027041993877745, 0.7027091556327009, 0.7026774323076271, 0.7026815988211652, 0.7026572954603537, 0.7026748647750883, 0.7026686269271197, 0.7026518596588097, 0.702647483656687, 0.702662519592924, 0.7026756777827297, 0.7026796512238378, 0.7026885635954013, 0.7026577180030261, 0.7026457900678819, 0.7026736933921516, 0.7026742738101767, 0.7026688586257027, 0.7026383167021201, 0.7026270133302678, 0.7026282060295737, 0.702618751347324, 0.7026202731210122, 0.7026403592042647, 0.7026468392305706, 0.702634761321117, 0.7026203424357164, 0.70261422639107, 0.702582720637191, 0.7025733811821246, 0.7025758039816375, 0.7025631131378249, 0.7025719999797738, 0.7025583939568619, 0.7025460802445344, 0.7025538911039589, 0.7025365280413326, 0.7025431460919921, 0.7025289410227842, 0.7025426821346946, 0.7025416785791817, 0.7025328911419951, 0.7025348431066621, 0.7025348445472993, 0.7025249767690463, 0.7025170439886922, 0.7025257674699931, 0.7025196324953776, 0.7025078890274176, 0.7024851804877189, 0.7024665954716717, 0.7024502445526423, 0.7024539474679065, 0.7024383669252945, 0.7024109902893886, 0.7024003929467403, 0.7023800546782054, 0.7023739231669168, 0.7023593233902187, 0.7023573891239234, 0.7023420415319702, 0.7023511558941565, 0.7023791593310689, 0.7023920331981112, 0.7024177885472609, 0.7024343963613507, 0.7024219759375282, 0.70243929068433, 0.7024332952105428, 0.7024081642809747, 0.7024183504237606, 0.7024002169055427, 0.702388648506698, 0.7023855539850347, 0.7023887898830443, 0.7024148448034738, 0.7024185516938842, 0.7024130216035381, 0.7024214628011342, 0.7024330053310259, 0.7024389730339506, 0.702453249583181, 0.7024815014458307, 0.7024679485757916, 0.7024526985019065, 0.7024394995272633, 0.7024476157417164, 0.7024598198001991, 0.7024528223797634, 0.7024517305792167, 0.7024293941299629, 0.702437101843684, 0.7024495610228764, 0.7024504872875058, 0.7024116687264127, 0.7024295360857656, 0.7024156112361114, 0.7024193977626322, 0.7024284602062831, 0.7024382517534177, 0.7024484170111097, 0.7024365702455156, 0.7024357300807195, 0.7024219804292532, 0.7024113906255239, 0.7024148368624107, 0.7024115815331403, 0.7024094936188708, 0.702404980930495, 0.702405017848852, 0.7024012368490883, 0.7024118108947167, 0.702414698482062, 0.7024288098388235, 0.7024235229220436, 0.7024258948545657, 0.7024327909043704, 0.7024405490695261, 0.7024513622386545, 0.7024490330228038, 0.7024380718469656, 0.7024248086264926, 0.7024230962420548, 0.7024249665023499, 0.7024205863553398, 0.7024270190336487, 0.7024454927701141, 0.7024612573523122, 0.7024691088959844, 0.702462440814061, 0.7024699690979612, 0.7024559647336258, 0.7024628103022152, 0.7024725123800737, 0.7024717182157282, 0.7024584957667525, 0.7024578085194086, 0.7024449715043123, 0.7024519842171061, 0.7024336305462423, 0.7024334399194849, 0.7024309778896298, 0.7024494079837789, 0.702453181126629, 0.7024555491427004, 0.7024428013425567, 0.7024378063206282, 0.7024575023396858, 0.7024551576243842, 0.7024508847113604, 0.7024645092903635, 0.7024692588896185, 0.7024597861313698, 0.7024666957956613, 0.7024613032727954, 0.7024454035604254, 0.7024525196900472, 0.7024594176028952, 0.7024569461243519, 0.7024514939582103, 0.7024421615032029, 0.7024420992524453, 0.7024493673586596, 0.7024482737359546, 0.7024562772462684, 0.7024668489986595, 0.7024615871993994, 0.7024732546346811, 0.7024904016349639, 0.7024887397527966, 0.7024798255628962, 0.7024997048434992, 0.7025047173754543, 0.7024761343496099, 0.7024769070563277, 0.7024692402067622, 0.702456724601734, 0.7024398283944807, 0.7024409042494466, 0.7024381916883847, 0.7024376000032627, 0.7024421083481669, 0.7024323652785222, 0.7024262008185426, 0.7024170394577928, 0.7024200396097493, 0.702425528724317, 0.7024238298553626, 0.7024232654492572, 0.7024101754023354, 0.7023966900298175, 0.7023979623530477, 0.7023953993680451, 0.7024019995459767, 0.702397691051182, 0.7023894559976771, 0.7023811023318268, 0.7023604698025188, 0.7023611256040377, 0.7023645075904796, 0.7023663813885888, 0.7023613667696242, 0.702355048588193, 0.7023678429904529, 0.70235396489809, 0.7023381448802376, 0.7023390552251009, 0.702341392346249, 0.7023266371293433, 0.7023050419197827, 0.7023154856673118, 0.7023118605768158, 0.7023334519912106, 0.7023372856778879, 0.7023443549972191, 0.702359541819782, 0.7023565568701935, 0.7023515464132061, 0.7023696270554444, 0.7023803353686215, 0.7024257439594427, 0.7024215915462525, 0.7024523961539803, 0.7024779565349196, 0.7024938340780321, 0.7025212799699466], 'valid auc-stdv': [0.005667076946261924, 0.004761585982533667, 0.005791353983787571, 0.006150146567768029, 0.005846775893604045, 0.004708067607953435, 0.004897051340110795, 0.004455728133338632, 0.004478262234053934, 0.0043099165501246485, 0.00418469955454785, 0.004098329259262075, 0.004200878694768774, 0.004318597558013408, 0.00424312251749983, 0.004290663735660158, 0.004138692910782545, 0.004283588615634429, 0.004340696796468608, 0.0044322858357607265, 0.00417962399117968, 0.004205787300818002, 0.004353743885793988, 0.004111211730524474, 0.0038882775889041306, 0.003892466272110164, 0.0039839858308431545, 0.004098167596223564, 0.004009727729954827, 0.003960124415278638, 0.004042097555345754, 0.003934895222017456, 0.004025593826286606, 0.004090576657849375, 0.0040893062145676414, 0.004186343321414783, 0.0042121159412693435, 0.004107074895875189, 0.004131226960032764, 0.00418393504295716, 0.0041514341722376225, 0.004218855762310829, 0.004176907774552127, 0.004294449942133395, 0.004209100270562748, 0.00426127794487066, 0.00429398268173686, 0.004249351764555344, 0.004305771184659053, 0.004367244534368577, 0.004443282297129674, 0.004465553419713813, 0.004499761322523349, 0.0043908541008775195, 0.004354618294601952, 0.0043445324033430655, 0.004265953730549663, 0.004239224538729642, 0.0042421305353661865, 0.004300375349993078, 0.004212129960778413, 0.004200169492907972, 0.004204976139997413, 0.00413692099487337, 0.004133740461272326, 0.004064116163635318, 0.004143675280706813, 0.004174076140961982, 0.004129847484166015, 0.004081283873345229, 0.004171184878584399, 0.004188578635930931, 0.004182359262398771, 0.004204752206792948, 0.004124498495532966, 0.004148767931474532, 0.004196235524213371, 0.004271856115232818, 0.004254288283658986, 0.00432302290343569, 0.004323332866008207, 0.004281069641460387, 0.00426981363359999, 0.004234578691020165, 0.004226103345959706, 0.004168692643673278, 0.004171332836444496, 0.004127134850104885, 0.0041201515860152345, 0.00410692265250628, 0.00411524305790602, 0.004069556203599885, 0.0041080913559199475, 0.0041383821392564175, 0.004101940004497746, 0.004063171467550022, 0.004067865890974894, 0.004024596210589906, 0.004004434869210533, 0.0040015897375078125, 0.004054270384066899, 0.004068624713631679, 0.004067637771343994, 0.0041020177285482325, 0.004129660583016998, 0.004100619764218326, 0.004067323517055592, 0.004017236648742122, 0.004005973277477534, 0.0040476427260234486, 0.004028420035213057, 0.004072870191846212, 0.0040386455019212905, 0.004004312748822378, 0.004002012391227809, 0.004026727017459462, 0.004039073552037596, 0.0040061076905236305, 0.003996824029855536, 0.003971895679787635, 0.00390138389854633, 0.003872776309296687, 0.0038531541422724028, 0.0038413787489560334, 0.0038188458808399404, 0.0038115388432972755, 0.0037746255446713182, 0.0037337951819327287, 0.003705272499631349, 0.003739440703910065, 0.0037449863280988945, 0.0037708392479007063, 0.0037457075766598696, 0.003740546763439783, 0.0037014432943049845, 0.003747317549989376, 0.0037800951450202144, 0.0037400316989184973, 0.003745712170084447, 0.0037635941970440958, 0.0037650328984379935, 0.00372263625731205, 0.0036721643378863377, 0.0036809304272176443, 0.0037825127961866566, 0.0037678292210598908, 0.003797770782808294, 0.003801922579831085, 0.0038109181848788557, 0.003829718922523012, 0.003906867647141285, 0.0038824420512202887, 0.0039001317096638128, 0.0038403496594166826, 0.003852353974160488, 0.003805660732752442, 0.003789501888336287, 0.0037764312528635467, 0.0038051166123113118, 0.003797888910547649, 0.0038315655372935095, 0.0038963200442638157, 0.0038885047569770987, 0.003967342068961934, 0.003954077391075076, 0.003965758954392296, 0.003959620091921422, 0.00394400775291064, 0.003977601605583309, 0.00398885836312116, 0.0040211885797613225, 0.0040285907038620054, 0.004042643884006259, 0.004039667435543597, 0.004049580083120923, 0.0040395863076887, 0.0040909251125907575, 0.004089195246193041, 0.004090347986072072, 0.00405981204981443, 0.004027790595348251, 0.00404319789142202, 0.0039975102233969885, 0.003938714963362005, 0.003921603803355481, 0.00394445059934087, 0.003925314916751213, 0.0038885593983842073, 0.003886713578755849, 0.003908986064415595, 0.0038761185101637338, 0.003871788305608768, 0.0039053354955959313, 0.0039039649476790357, 0.003919034811112491, 0.0038946071445481747, 0.0038884801201847156, 0.0038840498994438793, 0.003915485792158663, 0.0038405523925705595, 0.0038468311822450777, 0.003827566216407743, 0.0038420670743268084, 0.0038443794891181433, 0.003846456228263691, 0.003882636947533395, 0.003851977175137173, 0.003793123654442815, 0.0038178044694104655, 0.003803644631489562, 0.00380011715337342, 0.0038140734080841184, 0.0038133724855896694, 0.003811690667398564, 0.0038501712127614555, 0.0037949035694827445, 0.003794258424791999, 0.0037914315755261277, 0.0038148704641859555, 0.0038651335378763014, 0.0038601016862386747, 0.0038402525562368337, 0.0038243561282243948, 0.0037473798444640416, 0.0037348656718077817, 0.0037332332651644, 0.0037169919599519333, 0.003748430790123103, 0.0036946707566411626, 0.0037173798998904432, 0.003745524000701757, 0.0037673408392452134, 0.0037493360604720755, 0.003764303258709813, 0.0037479766010614095, 0.0037303922130374734, 0.0036806896328501356, 0.003707458464131467, 0.0037166971996571135, 0.0037029282713185235, 0.003645437892276976, 0.0036242104281211406, 0.0035644402265532036, 0.0035754067011345442, 0.003590265116610734, 0.003550081514778882, 0.0035144390619035867, 0.0035306891152024978, 0.0035621176519859384, 0.0035808894872913586, 0.0036088669304695582, 0.003552987223252156, 0.0035000106292668587, 0.0034730612104245536, 0.0034582440085443537, 0.003471985066293414, 0.003467693292963366, 0.003475860120347668, 0.0034904729591259476, 0.003497217654722159, 0.0034533724701093886, 0.0034451585968847016, 0.0034847635244405306, 0.0035412990084943623, 0.0035543574475633043, 0.0035419321016274983, 0.0035090312553953465, 0.0034881072328747237, 0.0034632735127174364, 0.003465808640688694, 0.00343369182862742, 0.0034412935304966286, 0.003453665540703592, 0.003466943233637003, 0.0034573799216766365, 0.003475150926422325, 0.0035164473490803456, 0.0034726548838277076, 0.0034777671139602545, 0.0035072681903596113, 0.003527533424544937, 0.0034736725442439122, 0.003495785514542456, 0.003479495670358561, 0.003498170290993724, 0.0034802441942936857, 0.003528079235787043, 0.003546582106722163, 0.003542398741239536, 0.003511741246055245, 0.0034565762352516106, 0.0034154771109770198, 0.0034210363599496683, 0.0034480228837844313, 0.003442742989241707, 0.0034535013336274246, 0.0034570422677709495, 0.003446091113510994, 0.003369718674108538, 0.0033286010871531324, 0.0032858057341764568, 0.0032571674892382693, 0.0032534932649524394, 0.0032091125531509483, 0.003200010278401084, 0.003207263707367789, 0.0031683116657694175, 0.00316244052536832, 0.003210574803800355, 0.0031773830144808685, 0.003197462742969666, 0.00324506102525418, 0.003229397033104056, 0.0032216009210952997, 0.0031622989140722953, 0.0031250762629482574, 0.003157440466495086, 0.003175073839943278, 0.0031501165062921533, 0.0031396327575748703, 0.003120455014819293, 0.0031458757518299726, 0.0031660009378057436, 0.003153821728631543, 0.0031150280069413166, 0.003107606291407552, 0.0031086250169841045, 0.003097019287919552, 0.0030872706652267205, 0.003083039480742626, 0.003090047957837946, 0.0030512335746033462, 0.003028427313689488, 0.0030324686916473127, 0.00306390093969995, 0.003063907781286285, 0.00306511382169567, 0.0030919163280382017, 0.0030765847012893806, 0.0031356587786703478, 0.003082605305925866, 0.0030728298674272208, 0.003075381533884317, 0.003041825992881176, 0.0030304650857627216, 0.0029997255324552977, 0.003016047719996343, 0.0030158269601700764, 0.003013315231918462, 0.0029774369653759276, 0.002968296888510319, 0.002971446016033951, 0.002960219399455962, 0.0029374242865939635, 0.002914222331953135, 0.0029087182175532725, 0.0028892482234937488, 0.0028935724949819054, 0.0029220139595048855, 0.0028784634102092034, 0.0028872281152885816, 0.002873078328926875, 0.0028761685973665283, 0.0028681981064278004, 0.0029227985933372386, 0.002918673006364689, 0.002859962405148056, 0.002860652112137231, 0.002820811477428616, 0.0028648686143456802, 0.002819692946699223, 0.002806547962468351, 0.002821369444390558, 0.002832019815956804, 0.002805360086196932, 0.0028328920504546995, 0.002834312468094073, 0.002824272414418999, 0.0027860879240375423, 0.0028295109072121226, 0.0028633301203744924, 0.0028386593671779897, 0.0028358412502449393, 0.0028508421887912896, 0.0028471865797890386, 0.0028477263741050088, 0.0028390836418751403, 0.002821518273574593, 0.002826627038058607, 0.002836048170370127, 0.002852361212850807, 0.0028564681218934103, 0.0028926556098620433, 0.002962808094841603, 0.002952735489784832, 0.002978210363287826, 0.002992801685748161, 0.00300969252534721, 0.0030163596657143466, 0.0030157338312943186, 0.003036674660725198, 0.0030044464593389116, 0.0029909545171672492, 0.002980716950728372, 0.002974271762257687, 0.002940480975164226, 0.0029407458663113393, 0.0029376343490870776, 0.0029289792077424642, 0.0029136797254164194, 0.002876883867228887, 0.002849561215817322, 0.0028237738210910336, 0.0027753440476315943, 0.0027530922265949632, 0.002727745958707351, 0.0026719260427571522, 0.0026692523914855363, 0.002699957967972388, 0.002662056496980135, 0.002624453497605592, 0.0026245059576315765, 0.0025737227201159127, 0.0025822071545930194, 0.0025548641259461203, 0.0025402323754372337, 0.0025302203002659637, 0.002504343257391143, 0.0025206148377889988, 0.002530351434511873, 0.0025199124823634313, 0.0025233821011966273, 0.0025086683734143276, 0.0025027236525375527, 0.0025160726681534105, 0.002492375694726258, 0.0025085877277683435, 0.0024878125983384494, 0.00245268762114218, 0.0024641850881851257, 0.0024840052114848345, 0.002480847155832332, 0.00246601845967432, 0.0024831698209031605, 0.002472078087894666, 0.0024948922523409067, 0.0025012386951957033, 0.0025109600015264097, 0.0025196365734919315, 0.0025382759456759442, 0.0025381295081732167, 0.0025367461876405336, 0.0025454294644494473, 0.002505862107153816, 0.002482869666614552, 0.002486627547880279, 0.0024666723240037605, 0.002464299435409296, 0.002476037120386982, 0.002453607992973365, 0.0024395582311085872, 0.002432865498279938, 0.0024551118164108893, 0.002469246956759451, 0.002447488698927287, 0.002411032047930965, 0.0024005665368162067, 0.002377291437979423, 0.002339314699359182, 0.002320197508571817, 0.00231202550192573, 0.002293233932727096, 0.0023063212181409936, 0.0022985064566506865, 0.002316073662672969, 0.00229197744503159, 0.0022601823500993166, 0.00223076816578166, 0.0022693647242326654, 0.002248638030584938, 0.002251659550938773, 0.002224115327305366, 0.0021856307798344427, 0.002208050417961145, 0.0022228878575608763, 0.0022508448811878897, 0.002248568736195354, 0.0022388039270128247, 0.0022335684782801464, 0.002230847087885537, 0.0022366152409057094, 0.002221886954882032, 0.0021988875265275495, 0.002185821140352632, 0.002175207136403206, 0.0021695733080205172, 0.00217230456409608, 0.0021711235392485734, 0.0021582292597661914, 0.00213958140586334, 0.002115741791909327, 0.0021163748755338224, 0.0021211165885205324, 0.002134049115899987, 0.0021453383792464373, 0.0022005357790446534, 0.0021979460912569383, 0.0022157609851605445, 0.002250721201039057, 0.0022500227336703493, 0.0022308395116894327, 0.002223831180569918, 0.002258055265903979, 0.0022785413547091217, 0.002285334787995231, 0.002253117256208193, 0.0022677985779108304, 0.002244743996369685, 0.002250318427639771, 0.0022768063680000186, 0.0022824225788398464, 0.002270110322481334, 0.0022707870048640968, 0.0023133224057392144, 0.0023398240881601657, 0.0023512909645984205, 0.0023812706056097707, 0.002353573953201139, 0.002352370265666721, 0.0023784364136474065, 0.002402284084809503, 0.0023939332297433016, 0.002389318832833364, 0.0023928937665211743, 0.002409443015306714, 0.002431907186381159, 0.00245938568037205, 0.00246691168274349, 0.0024456227921115826, 0.002443400418412943, 0.002445174632568258, 0.0024240406606025723, 0.002445234690333975, 0.002450394690273328, 0.0024648651121665223, 0.0024642722833247296, 0.0024645724651249368, 0.0024295293990377054, 0.0024394625683737377, 0.0024343947901923533, 0.0024396881351299083, 0.0024305732331924928, 0.0024099751966762083, 0.002412652212786737, 0.0024110508385520764, 0.0024092709258024805, 0.002396306469869536, 0.002382454653110427, 0.0023762854447674717, 0.002372377885868043, 0.00238010445406259, 0.002336960441367823, 0.0023535974216567536, 0.002342602301788033, 0.0023419942874957135, 0.002314888374166001, 0.002332773242145818, 0.0023133651286928645, 0.002310452476877071, 0.0022956066289455423, 0.0022959716828564953, 0.0022988760291034133, 0.0022813849480083402, 0.0022745693582793547, 0.0022813652529506292, 0.0022753090968338284, 0.0022682642812838867, 0.002241007727507693, 0.0022458104498743006, 0.002256698497550816, 0.0022299446957474802, 0.002229862541213319, 0.0022513674896592127, 0.0022763772797510973, 0.002275302040770891, 0.0022743706769473124, 0.002294622151202987, 0.0022798154413859665, 0.002268686701042174, 0.0022358623575722152, 0.002235653189387565, 0.002219958866937045, 0.0022363153852152664, 0.0022242132301083547, 0.002219386263289563, 0.002208515864180593, 0.00220340549003651, 0.002194204279144099, 0.0021700002418165365, 0.0021762530659384504, 0.0021811469688688806, 0.0021830295702689032, 0.002149217745361126, 0.0021462191922740374, 0.00215202785390088, 0.002138606359651181, 0.002139913948384182, 0.0021491220692073524, 0.002144465644333624, 0.0021286337667937734, 0.0021328363705749535, 0.0021335142489514324, 0.0021227896112665768, 0.00213170700681054, 0.0021613944423270405, 0.0021564314767972843, 0.002128420659220029, 0.0021465776030690654, 0.0021441924697581873, 0.002137526164556048, 0.002143293899462145, 0.0021267342698419018, 0.0021048085281987075, 0.0021000049254771714, 0.002110518929896962, 0.00208141108079366, 0.0020806306115076457, 0.002075846631109504, 0.0020654762906950844, 0.002063172858214825, 0.0020552302464706992, 0.0020679934705387527, 0.0020585893920672515, 0.0020465403456583543, 0.002087103945715582, 0.002064076644245394, 0.0020540176441566664, 0.002069800891524565, 0.0020751950958468376, 0.002057197649991607, 0.0020509601687594796, 0.0020686924536493186, 0.0020894541093991487, 0.002098854768899783, 0.002095789086364474, 0.0020881927777708365, 0.0020651216500510126, 0.0020593653919684907, 0.0020673940508444434, 0.0020607537751413754, 0.0020590737002317617, 0.002042380591010261, 0.0020501858458040903, 0.002097519495030702, 0.0020815741319049666, 0.0020814200565111855, 0.0020811754780255486, 0.0021064923612923595, 0.0021388587435979592, 0.002143041482199374, 0.002147909967100062, 0.0021400355280404085, 0.0021376803523883363, 0.0021240072340917263, 0.002138956220070983, 0.0021289711780022764, 0.0021355268274464248, 0.002109734566218426, 0.00208779343666567, 0.002052408120683491, 0.002049093010822905, 0.002069793661864651, 0.0020497442729077975, 0.0020448541564408937, 0.0020412089420217086, 0.0020329599653886973, 0.002002420775608407, 0.0019959002317240186, 0.0020022064782428063, 0.002008792749722437, 0.002005077056733813, 0.002018226658604973, 0.0020100303707475137, 0.0020131561401617766, 0.0020096994237178193, 0.0020009771643957623, 0.00197396454342278, 0.001966527804892307, 0.0019700275891146443, 0.001950108706200403, 0.0019433164128091897, 0.0019382283089682108, 0.001915276034327668, 0.001921298906979166, 0.0019299284581531678, 0.0019189555475259306, 0.0019044595832061766, 0.0018889700012130637, 0.0018879793326513456, 0.0018903813021440882, 0.0019028471873302948, 0.0018967295346535972, 0.0018822890363584233, 0.0018779558963813146, 0.0018467722152772823, 0.0018406791430489856, 0.001864461626746941, 0.0018590849812619003, 0.0018440329456993214, 0.0018370045186713645, 0.0018266370712768881, 0.0018007974581887365, 0.0017992449521748216, 0.0017919930525330274, 0.0017697111990214267, 0.001757515362331034, 0.0017445230292604536, 0.001736679332698933, 0.0017328225999932467, 0.0016923162827213215, 0.0016876295898095336, 0.001675571460689606, 0.0016769864258443382, 0.001694712243001373, 0.0016545419494593818, 0.0016762488029771886, 0.0016837495733394597, 0.0016848388833853136, 0.00166065184683054, 0.0016447283942861018, 0.0016518526981176005, 0.001671687229609362, 0.0016662623355156831, 0.0016529766215276145, 0.0016225305005986024, 0.001632234610443049, 0.0016490532698433566, 0.0016517129439309443, 0.0016465638500687244, 0.001641099208249183, 0.0016227238725363767, 0.0016251671540656431, 0.0015984859602793514, 0.0016059152400008663, 0.0016217910124655748, 0.0016115466358250874, 0.0015952226083563235, 0.001581100424612268, 0.0015493059086467754, 0.0015571665512799677, 0.0015822788693293332, 0.001598388048427907, 0.0015706367023110217, 0.0015607039393834307, 0.0015681383195290854, 0.0015690919885824955, 0.0015546072578579654, 0.0015705292645209808, 0.0015623225882471226, 0.0015682423031336626, 0.0015912481340080926, 0.0016064608410997854, 0.001583588943834918, 0.0015859928514091984, 0.0016007813170623318, 0.0016148203054385918, 0.0016132193197144669, 0.0015921303658795918, 0.001577564836497182, 0.0015473468006429792, 0.0015357240718763144, 0.001528349649747533, 0.0015253065308830991, 0.0015148056786862033, 0.0015051314449568839, 0.0014909687245505598, 0.001476454309634253, 0.0014609756589386155, 0.001434432280554891, 0.0014314152163803103, 0.0014004717510508818, 0.001382789107899687, 0.0013700661543671895, 0.0013592723883336182, 0.001345099735425278, 0.001348326473321576, 0.0013369342524654766, 0.0013211412986513502, 0.0013071395677958714, 0.001330078508354417, 0.001301795255700282, 0.0012933574433007466, 0.0012588275381090546, 0.0012474613901792718, 0.0012519945650565382, 0.0012372232702187204, 0.0012339573618460793, 0.001182977421412201, 0.0011683683746615164, 0.0011605676165628797, 0.0011522282722433365, 0.001171938522682952, 0.0011856301382084794, 0.0011420961876545364, 0.0011350764843986879, 0.001130913794042657, 0.0011354929032826079, 0.001171835505384081, 0.0011775917681991592, 0.0011740235105328236, 0.0011735738090023827, 0.0011663906775673954, 0.001171687143064365, 0.001195938057289124, 0.0011708685890439764, 0.0011633796522330484, 0.0011523356451995266, 0.0011767412524339904, 0.0012013880859377726, 0.0011784936467004277, 0.0011654678072429083, 0.0011498913015225752, 0.0011499573320014472, 0.0011700395130737303, 0.0011661132771570684, 0.0011561379750721042, 0.001175073856008384, 0.001169703081047879, 0.001187469562736785, 0.0011954985428908908, 0.0011992631047265732, 0.0012080300289467423, 0.001195928784396076, 0.0012059099232459723, 0.0012322933750063935, 0.0012358959366714673, 0.0012029101644262655, 0.0012005806746542699, 0.0012018373666727435, 0.001202803160659816, 0.0012041418465711543, 0.0011811696416016548, 0.0012000983898992387, 0.0012065274752485287, 0.001248372538483126, 0.001249047050448166, 0.0012519077186184386, 0.0012897420547527347, 0.0012949201137345085, 0.0013197290625929126, 0.001322524569653209, 0.0013254760399546081, 0.0013258839981079853, 0.0013486608415783732, 0.0013629457031966855, 0.0013820470550788624, 0.0013997758518134915, 0.0014053928682248244, 0.0014176725688170076, 0.001424018979327376, 0.0014278791984739072, 0.001455128826409251, 0.0014585741176887297, 0.0014574661284245866, 0.0014457252849221596, 0.0014397724073598498, 0.0014522601898839553, 0.001444208570841425, 0.001473751680701531, 0.0014516964204261424, 0.001466033762659926, 0.001477438608813147, 0.0014563309484154113, 0.0014723334619259855, 0.001468377627562167, 0.0014881260612203894, 0.0014592730411221119, 0.001480705836311294, 0.001512872530558196, 0.0015045647115709188, 0.0014983487071710182, 0.0014999859212908634, 0.0015071438229812778, 0.0015146133351218406, 0.0015258168653410939, 0.0015270197639112404, 0.0015252741974241435, 0.0015319893650302466, 0.0015290842001487318, 0.0015399002847361581, 0.0015495000187718162, 0.001553305883675123, 0.0015494032099399827, 0.0015560828689583964, 0.0015692011848724562, 0.0015917838210531368, 0.0015937633671263053, 0.0015822738758879162, 0.0015797399647850545, 0.0015974294743732347, 0.0016089944007070052, 0.001653870083564321, 0.001654414596688913, 0.001642866602181641, 0.0016624043863735039, 0.0016667954506831176, 0.001660954257695743, 0.0016738917758309922, 0.0016892607714939047, 0.0017138837194192508, 0.0017320329329305161, 0.0017406638533158394, 0.0017549429277918004, 0.0017704775627183811, 0.0017607177548978718, 0.001790202358054694, 0.0018365996421679878, 0.0018341763087485008, 0.0018478849053026852, 0.001876714488406479, 0.0018837405665193445, 0.0018734784650956961, 0.001870138619655665, 0.0018463407209285232, 0.0018788700368653963, 0.0018915707549438734, 0.001908126968563407, 0.0019236283238661354, 0.0019478174793266208, 0.0019411201004106108, 0.00193152800880791, 0.0019404537570829638, 0.001947200624019643, 0.0019410628975755902, 0.0019432278100450875, 0.0019492723108755796, 0.0019358485090383208, 0.0019412616376017235, 0.001932219417596351, 0.001942490591238376, 0.001942448466653601, 0.001934933604316997, 0.001933131341286598, 0.0019323856446864212, 0.001936475933162688, 0.001925624164695238, 0.0019333723879641374, 0.0019440873323626582, 0.001950869071034185, 0.0019618681759170607, 0.0019450682346376308, 0.0019603208923672758, 0.001961111221090028, 0.001961765275657745, 0.0019658590873722083, 0.0019765045793331314, 0.001976322032140969, 0.0019756756983070898, 0.001989775266641187, 0.001997338266389488, 0.002008400256728744, 0.00201739058384077, 0.002020438436467854, 0.0020350923981086164, 0.002067450931891626, 0.0020822345789500867, 0.002108791440968004, 0.00212429286951276, 0.002149532680111879, 0.002152439090443777, 0.0021581862003864517, 0.0021861259473480443, 0.002185071245823906, 0.0021860432214803336, 0.00219042657476193, 0.002197362467608926, 0.002204917748086567, 0.002201671803203264, 0.002204187553942308, 0.002205981451175127, 0.0022262258603348906, 0.002222298646748437, 0.00220331851784142, 0.0021943094079587954, 0.002183756479914496, 0.0021851473357355415, 0.002172703566914441, 0.002151968854774502, 0.002150332910598914, 0.0021790741587168956, 0.0021715239996766026, 0.0021572217176691133, 0.0021793725198762178, 0.002154205958739517, 0.0021678439900075356, 0.002161242692803593, 0.0021807460035774425, 0.0022073252884315547, 0.0022501969195285426, 0.002292471183823406]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2068\\2623645567.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                 })\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlgbBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutility_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_subscribers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\logger.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, event, instance)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[0mcolour\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mColours\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpurple\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_new_max\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mColours\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolour\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_header_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\logger.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, instance, colour)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_constrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"allowed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\logger.py\u001b[0m in \u001b[0;36m_format_number\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 )\n\u001b[0;32m     41\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             s = \"{x:<{s}.{p}}\".format(\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_cell_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "lgbBO.maximize(n_iter=10, init_points=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdkxhhST-KZS"
   },
   "source": [
    " **<font color='teal'> Print the best result by using the '.max' function.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:49:01.513767Z",
     "start_time": "2019-04-22T15:49:01.509392Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oc8z6mfy-KZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': None,\n",
       " 'params': {'lambda_l1': 0.045349382790901185,\n",
       "  'lambda_l2': 0.019248866985360075,\n",
       "  'max_depth': 30.829416653918415,\n",
       "  'min_child_samples': 1076.9831360399294,\n",
       "  'min_data_in_leaf': 334.3350823066255,\n",
       "  'num_leaves': 977.0935678892608}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:50:29.049881Z",
     "start_time": "2019-04-22T15:50:29.045908Z"
    },
    "colab_type": "text",
    "id": "J5LAydKC-KZW"
   },
   "source": [
    "Review the process at each step by using the '.res[0]' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:51:01.001688Z",
     "start_time": "2019-04-22T15:51:00.997484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X1ttZmrI-KZX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': None,\n",
       " 'params': {'lambda_l1': 0.045349382790901185,\n",
       "  'lambda_l2': 0.019248866985360075,\n",
       "  'max_depth': 30.829416653918415,\n",
       "  'min_child_samples': 1076.9831360399294,\n",
       "  'min_data_in_leaf': 334.3350823066255,\n",
       "  'num_leaves': 977.0935678892608}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bayesian_optimization_exercise.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
